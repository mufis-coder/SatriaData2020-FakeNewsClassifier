{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kata = pd.read_csv('D:\\zzKoding\\Data Sains - Machine Learning - AI\\Lomba\\Satria Data 2020\\Data\\data_kata_csv.csv')\n",
    "data_kata_lema = [x for x in data_kata.lema]\n",
    "words = list(data_kata.lema)\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "import itertools\n",
    "dict(itertools.islice(WORDS.items(), 10))\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi\n",
    "# stopwords removal\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "stop = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "#stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ditendang\n",
      "tendang\n"
     ]
    }
   ],
   "source": [
    "sentence = 'ditendang'\n",
    "print(sentence)\n",
    "sentence = stemmer.stem(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah', 'lg', 'deh', 'toh', 'piye', 'prettt', 'tak', 'sblm', 'dlm', 'sekedar'])\n",
    "stop_words = list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stopw = list()\n",
    "def HiSed(df, perintah, stop_words, lema, correction, stemmer, add_stopw):\n",
    "    if perintah == 'sederhana':\n",
    "        df_return = pd.DataFrame()\n",
    "    elif perintah == 'hitung':\n",
    "        dfCounts_return = dict()\n",
    "        \n",
    "    for index, row in df.iterrows():\n",
    "        filter_sentence = ''\n",
    "        sentence = row['total']\n",
    "        # Cleaning dengan regular expression\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "        # Tokenization\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Menghilangkan stopwords\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        if perintah == 'sederhana':\n",
    "            # Lower case\n",
    "            for words in words:\n",
    "                kata_temp = words.lower()\n",
    "                if kata_temp in lema:\n",
    "                    filter_sentence = filter_sentence  + ' ' + kata_temp\n",
    "                else:\n",
    "                    cor_kat = correction(kata_temp)\n",
    "                    if cor_kat != kata_temp:\n",
    "                        filter_sentence = filter_sentence  + ' ' + cor_kat\n",
    "                    else:\n",
    "                        ste_kat = stemmer.stem(kata_temp)\n",
    "                        if ste_kat != kata_temp:\n",
    "                            filter_sentence = filter_sentence  + ' ' + ste_kat\n",
    "                        else:\n",
    "                            if kata_temp not in add_stopw:\n",
    "                                add_stopw.append(kata_temp)\n",
    "            df_return.loc[index, 'total'] = filter_sentence\n",
    "        elif perintah == 'hitung':\n",
    "            for word in words:\n",
    "                if word.lower() in dfCounts_return:\n",
    "                    dfCounts_return[word.lower()] += 1\n",
    "                else:\n",
    "                    dfCounts_return[word.lower()] = 1\n",
    "            \n",
    "    if perintah == 'sederhana':\n",
    "        return df_return, add_stopw\n",
    "    elif perintah == 'hitung':\n",
    "        return dfCounts_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>judul</th>\n",
       "      <th>narasi</th>\n",
       "      <th>nama file gambar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-17 00:00:00</td>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "      <td>A caller to a radio talk show recently shared ...</td>\n",
       "      <td>71.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-17 00:00:00</td>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "      <td>Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...</td>\n",
       "      <td>461.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>495</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-13 00:00:00</td>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "      <td>Jokowi adalah presiden terbaik dlm sejarah ban...</td>\n",
       "      <td>495.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-08 00:00:00</td>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "      <td>Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...</td>\n",
       "      <td>550.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>681</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-24 00:00:00</td>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin</td>\n",
       "      <td>Kadrun kalo lihat foto ini panas dingin . .</td>\n",
       "      <td>681.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>736</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-17 00:00:00</td>\n",
       "      <td>event promo smartphone JNE 2020 spesial di bul...</td>\n",
       "      <td>selamat siang teman teman fb ku semuanyaü§©,cuma...</td>\n",
       "      <td>736.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1161</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-05 00:00:00</td>\n",
       "      <td>MEREKA SUDAH MEMPERSIAPKAN DIRI DENGAN BAIK UN...</td>\n",
       "      <td>LIHATLAH MEREKA SUDAH MEMPERSIAPKAN DIRI DENGA...</td>\n",
       "      <td>1161.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1597</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24 00:00:00</td>\n",
       "      <td>Foto Presiden Italia menangis karena tak cukup...</td>\n",
       "      <td>Italia punya fasilitas perawatan kesehatan ter...</td>\n",
       "      <td>1597.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2098</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-09 00:00:00</td>\n",
       "      <td>Kapolres Timor Tengah Utara , Nusa Tenggara Ti...</td>\n",
       "      <td>Polisi goblok. Bukanya cpet‚Äù d tolongin malah ...</td>\n",
       "      <td>2098.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2226</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-24 00:00:00</td>\n",
       "      <td>Video Polisi china telah menganiaya wanita uig...</td>\n",
       "      <td>Polisi china telah menganiaya wanita uighur le...</td>\n",
       "      <td>2226.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  label              tanggal  \\\n",
       "0    71      1  2020-08-17 00:00:00   \n",
       "1   461      1  2020-07-17 00:00:00   \n",
       "2   495      1  2020-07-13 00:00:00   \n",
       "3   550      1  2020-07-08 00:00:00   \n",
       "4   681      1  2020-06-24 00:00:00   \n",
       "5   736      1  2020-06-17 00:00:00   \n",
       "6  1161      1  2020-05-05 00:00:00   \n",
       "7  1597      1  2020-03-24 00:00:00   \n",
       "8  2098      1  2020-01-09 00:00:00   \n",
       "9  2226      1  2019-12-24 00:00:00   \n",
       "\n",
       "                                               judul  \\\n",
       "0  Pemakaian Masker Menyebabkan Penyakit Legionna...   \n",
       "1  Instruksi Gubernur Jateng tentang penilangan  ...   \n",
       "2  Foto Jim Rohn: Jokowi adalah presiden terbaik ...   \n",
       "3  ini bukan politik, tapi kenyataan Pak Jokowi b...   \n",
       "4       Foto Kadrun kalo lihat foto ini panas dingin   \n",
       "5  event promo smartphone JNE 2020 spesial di bul...   \n",
       "6  MEREKA SUDAH MEMPERSIAPKAN DIRI DENGAN BAIK UN...   \n",
       "7  Foto Presiden Italia menangis karena tak cukup...   \n",
       "8  Kapolres Timor Tengah Utara , Nusa Tenggara Ti...   \n",
       "9  Video Polisi china telah menganiaya wanita uig...   \n",
       "\n",
       "                                              narasi nama file gambar  \n",
       "0  A caller to a radio talk show recently shared ...           71.jpg  \n",
       "1  Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...          461.png  \n",
       "2  Jokowi adalah presiden terbaik dlm sejarah ban...          495.png  \n",
       "3  Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...          550.png  \n",
       "4        Kadrun kalo lihat foto ini panas dingin . .          681.jpg  \n",
       "5  selamat siang teman teman fb ku semuanyaü§©,cuma...          736.png  \n",
       "6  LIHATLAH MEREKA SUDAH MEMPERSIAPKAN DIRI DENGA...         1161.png  \n",
       "7  Italia punya fasilitas perawatan kesehatan ter...         1597.png  \n",
       "8  Polisi goblok. Bukanya cpet‚Äù d tolongin malah ...         2098.jpg  \n",
       "9  Polisi china telah menganiaya wanita uighur le...         2226.png  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('D:\\zzKoding\\Data Sains - Machine Learning - AI\\Lomba\\Satria Data 2020\\Data\\Salinan Data Latih BDC.xlsx')\n",
    "df_judul = df['judul']\n",
    "df_narasi = df['narasi']\n",
    "df_label = df['label']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>judul</th>\n",
       "      <th>narasi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "      <td>A caller to a radio talk show recently shared ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "      <td>Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>495</td>\n",
       "      <td>1</td>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "      <td>Jokowi adalah presiden terbaik dlm sejarah ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "      <td>Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>681</td>\n",
       "      <td>1</td>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin</td>\n",
       "      <td>Kadrun kalo lihat foto ini panas dingin . .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>736</td>\n",
       "      <td>1</td>\n",
       "      <td>event promo smartphone JNE 2020 spesial di bul...</td>\n",
       "      <td>selamat siang teman teman fb ku semuanyaü§©,cuma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1161</td>\n",
       "      <td>1</td>\n",
       "      <td>MEREKA SUDAH MEMPERSIAPKAN DIRI DENGAN BAIK UN...</td>\n",
       "      <td>LIHATLAH MEREKA SUDAH MEMPERSIAPKAN DIRI DENGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1597</td>\n",
       "      <td>1</td>\n",
       "      <td>Foto Presiden Italia menangis karena tak cukup...</td>\n",
       "      <td>Italia punya fasilitas perawatan kesehatan ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2098</td>\n",
       "      <td>1</td>\n",
       "      <td>Kapolres Timor Tengah Utara , Nusa Tenggara Ti...</td>\n",
       "      <td>Polisi goblok. Bukanya cpet‚Äù d tolongin malah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2226</td>\n",
       "      <td>1</td>\n",
       "      <td>Video Polisi china telah menganiaya wanita uig...</td>\n",
       "      <td>Polisi china telah menganiaya wanita uighur le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  label                                              judul  \\\n",
       "0    71      1  Pemakaian Masker Menyebabkan Penyakit Legionna...   \n",
       "1   461      1  Instruksi Gubernur Jateng tentang penilangan  ...   \n",
       "2   495      1  Foto Jim Rohn: Jokowi adalah presiden terbaik ...   \n",
       "3   550      1  ini bukan politik, tapi kenyataan Pak Jokowi b...   \n",
       "4   681      1       Foto Kadrun kalo lihat foto ini panas dingin   \n",
       "5   736      1  event promo smartphone JNE 2020 spesial di bul...   \n",
       "6  1161      1  MEREKA SUDAH MEMPERSIAPKAN DIRI DENGAN BAIK UN...   \n",
       "7  1597      1  Foto Presiden Italia menangis karena tak cukup...   \n",
       "8  2098      1  Kapolres Timor Tengah Utara , Nusa Tenggara Ti...   \n",
       "9  2226      1  Video Polisi china telah menganiaya wanita uig...   \n",
       "\n",
       "                                              narasi  \n",
       "0  A caller to a radio talk show recently shared ...  \n",
       "1  Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...  \n",
       "2  Jokowi adalah presiden terbaik dlm sejarah ban...  \n",
       "3  Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...  \n",
       "4        Kadrun kalo lihat foto ini panas dingin . .  \n",
       "5  selamat siang teman teman fb ku semuanyaü§©,cuma...  \n",
       "6  LIHATLAH MEREKA SUDAH MEMPERSIAPKAN DIRI DENGA...  \n",
       "7  Italia punya fasilitas perawatan kesehatan ter...  \n",
       "8  Polisi goblok. Bukanya cpet‚Äù d tolongin malah ...  \n",
       "9  Polisi china telah menganiaya wanita uighur le...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df = df.drop(['tanggal', 'nama file gambar'], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4231 entries, 0 to 4230\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      4231 non-null   int64 \n",
      " 1   label   4231 non-null   int64 \n",
      " 2   judul   4231 non-null   object\n",
      " 3   narasi  4231 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 132.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3465\n",
       "0     766\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>judul</th>\n",
       "      <th>narasi</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "      <td>A caller to a radio talk show recently shared ...</td>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "      <td>Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...</td>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>495</td>\n",
       "      <td>1</td>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "      <td>Jokowi adalah presiden terbaik dlm sejarah ban...</td>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "      <td>Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...</td>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>681</td>\n",
       "      <td>1</td>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin</td>\n",
       "      <td>Kadrun kalo lihat foto ini panas dingin . .</td>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  label                                              judul  \\\n",
       "0   71      1  Pemakaian Masker Menyebabkan Penyakit Legionna...   \n",
       "1  461      1  Instruksi Gubernur Jateng tentang penilangan  ...   \n",
       "2  495      1  Foto Jim Rohn: Jokowi adalah presiden terbaik ...   \n",
       "3  550      1  ini bukan politik, tapi kenyataan Pak Jokowi b...   \n",
       "4  681      1       Foto Kadrun kalo lihat foto ini panas dingin   \n",
       "\n",
       "                                              narasi  \\\n",
       "0  A caller to a radio talk show recently shared ...   \n",
       "1  Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...   \n",
       "2  Jokowi adalah presiden terbaik dlm sejarah ban...   \n",
       "3  Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...   \n",
       "4        Kadrun kalo lihat foto ini panas dingin . .   \n",
       "\n",
       "                                               total  \n",
       "0  Pemakaian Masker Menyebabkan Penyakit Legionna...  \n",
       "1  Instruksi Gubernur Jateng tentang penilangan  ...  \n",
       "2  Foto Jim Rohn: Jokowi adalah presiden terbaik ...  \n",
       "3  ini bukan politik, tapi kenyataan Pak Jokowi b...  \n",
       "4  Foto Kadrun kalo lihat foto ini panas dingin K...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total']=df['judul'] + ' ' + df['narasi']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>judul</th>\n",
       "      <th>narasi</th>\n",
       "      <th>nama file gambar</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238057</td>\n",
       "      <td>2020-07-13 00:00:00</td>\n",
       "      <td>Narasi Tito Karnavian Berideologi Komunis Kare...</td>\n",
       "      <td>TITO KARNIVAN ITU BERIDIOLOGI KOMUNIS DIA BISA...</td>\n",
       "      <td>238057.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238158</td>\n",
       "      <td>2020-07-06 00:00:00</td>\n",
       "      <td>Anies: Seberat beratnya Pekerjaan Akan terasa ...</td>\n",
       "      <td>Seberat beratnya Pekerjaan Akan terasa ringan ...</td>\n",
       "      <td>238158.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238865</td>\n",
       "      <td>2020-04-22 00:00:00</td>\n",
       "      <td>Hindu di india Melemparkan Patung Buatan Merek...</td>\n",
       "      <td>Hindu di india melemparkan patung buatan merek...</td>\n",
       "      <td>238865.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248298</td>\n",
       "      <td>2019-10-22 00:00:00</td>\n",
       "      <td>RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...</td>\n",
       "      <td>Mulai Hari ini di RSCM mulai diPraktekkan Peny...</td>\n",
       "      <td>248298.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255176</td>\n",
       "      <td>2020-05-01 00:00:00</td>\n",
       "      <td>Permohonan Kelonggaran Angsuran ke OJK</td>\n",
       "      <td>Untuk sekedar info, Bagi anda yg punya ansuran...</td>\n",
       "      <td>255176.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID              tanggal  \\\n",
       "0  238057  2020-07-13 00:00:00   \n",
       "1  238158  2020-07-06 00:00:00   \n",
       "2  238865  2020-04-22 00:00:00   \n",
       "3  248298  2019-10-22 00:00:00   \n",
       "4  255176  2020-05-01 00:00:00   \n",
       "\n",
       "                                               judul  \\\n",
       "0  Narasi Tito Karnavian Berideologi Komunis Kare...   \n",
       "1  Anies: Seberat beratnya Pekerjaan Akan terasa ...   \n",
       "2  Hindu di india Melemparkan Patung Buatan Merek...   \n",
       "3  RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...   \n",
       "4             Permohonan Kelonggaran Angsuran ke OJK   \n",
       "\n",
       "                                              narasi nama file gambar  label  \n",
       "0  TITO KARNIVAN ITU BERIDIOLOGI KOMUNIS DIA BISA...       238057.jpg      1  \n",
       "1  Seberat beratnya Pekerjaan Akan terasa ringan ...       238158.jpg      1  \n",
       "2  Hindu di india melemparkan patung buatan merek...       238865.jpg      1  \n",
       "3  Mulai Hari ini di RSCM mulai diPraktekkan Peny...       248298.jpg      1  \n",
       "4  Untuk sekedar info, Bagi anda yg punya ansuran...       255176.jpg      1  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t = pd.read_excel('D:\\zzKoding\\Data Sains - Machine Learning - AI\\Lomba\\Satria Data 2020\\Data\\Data Uji BDC.xlsx')\n",
    "df_t_judul = df_t['judul']\n",
    "df_t_narasi = df_t['narasi']\n",
    "df_t_label = df_t['label']\n",
    "df_t_id = df_t['ID']\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>judul</th>\n",
       "      <th>narasi</th>\n",
       "      <th>label</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238057</td>\n",
       "      <td>Narasi Tito Karnavian Berideologi Komunis Kare...</td>\n",
       "      <td>TITO KARNIVAN ITU BERIDIOLOGI KOMUNIS DIA BISA...</td>\n",
       "      <td>1</td>\n",
       "      <td>Narasi Tito Karnavian Berideologi Komunis Kare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238158</td>\n",
       "      <td>Anies: Seberat beratnya Pekerjaan Akan terasa ...</td>\n",
       "      <td>Seberat beratnya Pekerjaan Akan terasa ringan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Anies: Seberat beratnya Pekerjaan Akan terasa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238865</td>\n",
       "      <td>Hindu di india Melemparkan Patung Buatan Merek...</td>\n",
       "      <td>Hindu di india melemparkan patung buatan merek...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hindu di india Melemparkan Patung Buatan Merek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248298</td>\n",
       "      <td>RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...</td>\n",
       "      <td>Mulai Hari ini di RSCM mulai diPraktekkan Peny...</td>\n",
       "      <td>1</td>\n",
       "      <td>RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255176</td>\n",
       "      <td>Permohonan Kelonggaran Angsuran ke OJK</td>\n",
       "      <td>Untuk sekedar info, Bagi anda yg punya ansuran...</td>\n",
       "      <td>1</td>\n",
       "      <td>Permohonan Kelonggaran Angsuran ke OJK Untuk s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              judul  \\\n",
       "0  238057  Narasi Tito Karnavian Berideologi Komunis Kare...   \n",
       "1  238158  Anies: Seberat beratnya Pekerjaan Akan terasa ...   \n",
       "2  238865  Hindu di india Melemparkan Patung Buatan Merek...   \n",
       "3  248298  RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...   \n",
       "4  255176             Permohonan Kelonggaran Angsuran ke OJK   \n",
       "\n",
       "                                              narasi  label  \\\n",
       "0  TITO KARNIVAN ITU BERIDIOLOGI KOMUNIS DIA BISA...      1   \n",
       "1  Seberat beratnya Pekerjaan Akan terasa ringan ...      1   \n",
       "2  Hindu di india melemparkan patung buatan merek...      1   \n",
       "3  Mulai Hari ini di RSCM mulai diPraktekkan Peny...      1   \n",
       "4  Untuk sekedar info, Bagi anda yg punya ansuran...      1   \n",
       "\n",
       "                                               total  \n",
       "0  Narasi Tito Karnavian Berideologi Komunis Kare...  \n",
       "1  Anies: Seberat beratnya Pekerjaan Akan terasa ...  \n",
       "2  Hindu di india Melemparkan Patung Buatan Merek...  \n",
       "3  RSCM Praktekkan Penyedotan Plug  Vena/Saluran ...  \n",
       "4  Permohonan Kelonggaran Angsuran ke OJK Untuk s...  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['total']=df_t['judul'] + ' ' + df_t['narasi']\n",
    "df_t = df_t.drop(['tanggal', 'nama file gambar'], axis=1)\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-7739d04739d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_stopw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHiSed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sederhana'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_kata_lema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_stopw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-172-91d95e911265>\u001b[0m in \u001b[0;36mHiSed\u001b[1;34m(df, perintah, stop_words, lema, correction, stemmer, add_stopw)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mfilter_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_sentence\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkata_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                     \u001b[0mcor_kat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkata_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcor_kat\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mkata_temp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                         \u001b[0mfilter_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_sentence\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcor_kat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-1689f5eb1af9>\u001b[0m in \u001b[0;36mcorrection\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;34m\"Most probable spelling correction for word.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-1689f5eb1af9>\u001b[0m in \u001b[0;36mcandidates\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;34m\"Generate possible spelling corrections for word.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medits2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-1689f5eb1af9>\u001b[0m in \u001b[0;36mknown\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-1689f5eb1af9>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-1689f5eb1af9>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0medits2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;34m\"All edits that are two edits away from `word`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_temp, add_stopw = HiSed(df, 'sederhana', stop_words, data_kata_lema, correction, stemmer, add_stopw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_stopw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "df['total'] = total_temp\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_temp2, add_stopw = HiSed(df_t, 'sederhana', stop_words, data_kata_lema, correction, stemmer, add_stopw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_t.head())\n",
    "df_t['total'] = total_temp2\n",
    "display(df_t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords.extend(add_stopw)\n",
    "stop_words = list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inisiasi x dan y\n",
    "x_train = df['total']\n",
    "y_train = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_t['total']\n",
    "y_test = df_t['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(max_df=1.0)\n",
    "\n",
    "#Fit and transform train set, transform test set\n",
    "tfidf_train=tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test=tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_array = tfidf_train.toarray()\n",
    "tfidf_test_array = tfidf_test.toarray()\n",
    "tfidf_train_df = pd.DataFrame(tfidf_train_array)\n",
    "tfidf_test_df = pd.DataFrame(tfidf_test_array)\n",
    "display(tfidf_train_df.head())\n",
    "display(tfidf_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(tfidf_train_df, y_train)\n",
    "pred = model.predict(tfidf_test_df)\n",
    "score = metrics.f1_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParamSendiri(judul, narasi, label, stop_words):\n",
    "    angka = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n",
    "    tanda = ['?', '!', '.', ',', ' ']\n",
    "    df_baru_return = pd.DataFrame()\n",
    "    for index, row in judul.iteritems():\n",
    "        kata_judul = row\n",
    "        kata_narasi = narasi[index]\n",
    "        \n",
    "        if row[0] in angka:\n",
    "            df_baru_return.loc[index, 'judul_awal_angka'] = 1\n",
    "        elif row[0] not in angka:\n",
    "            df_baru_return.loc[index, 'judul_awal_angka'] = 0\n",
    "        if kata_narasi[0] in angka:\n",
    "            df_baru_return.loc[index, 'narasi_awal_angka'] = 1\n",
    "        elif kata_narasi[0] not in angka:\n",
    "            df_baru_return.loc[index, 'narasi_awal_angka'] = 0\n",
    "        \n",
    "        temp = 0\n",
    "        for x in kata_judul:\n",
    "            if x in tanda:\n",
    "                temp += 1\n",
    "        jumlah_tanda_judul = temp\n",
    "        df_baru_return.loc[index, 'judul_jumlah_tanda'] = jumlah_tanda_judul\n",
    "        temp = 0\n",
    "        for x in kata_narasi:\n",
    "            if x in tanda:\n",
    "                temp += 1\n",
    "        jumlah_tanda_narasi = temp\n",
    "        df_baru_return.loc[index, 'narasi_jumlah_tanda'] = jumlah_tanda_narasi\n",
    "        \n",
    "        kata_judul_bersih = re.sub(r'[^\\w\\s]', '', kata_judul)\n",
    "        kata_judul_token = nltk.word_tokenize(kata_judul_bersih)\n",
    "        jumlah_kata_judul = len(kata_judul_token)\n",
    "        df_baru_return.loc[index, 'judul_jumlah_kata'] = jumlah_kata_judul\n",
    "        if (jumlah_tanda_judul - jumlah_kata_judul) >= 0:\n",
    "            df_baru_return.loc[index, 'judul_jumlah_tanda_tak_wajar'] = (jumlah_tanda_judul - jumlah_kata_judul)\n",
    "        elif (jumlah_tanda_judul - jumlah_kata_judul) < 0:\n",
    "            df_baru_return.loc[index, 'judul_jumlah_tanda_tak_wajar'] = 0\n",
    "        kata_narasi_bersih = re.sub(r'[^\\w\\s]', '', kata_narasi)\n",
    "        kata_narasi_token =  nltk.word_tokenize(kata_narasi_bersih)\n",
    "        jumlah_kata_narasi = len(kata_narasi_token)\n",
    "        df_baru_return.loc[index, 'narasi_jumlah_kata'] = jumlah_kata_narasi\n",
    "        if (jumlah_tanda_narasi - jumlah_kata_narasi) >= 0:\n",
    "            df_baru_return.loc[index, 'narasi_jumlah_tanda_tak_wajar'] = (jumlah_tanda_narasi - jumlah_kata_narasi)\n",
    "        elif (jumlah_tanda_narasi - jumlah_kata_narasi) < 0:\n",
    "            df_baru_return.loc[index, 'narasi_jumlah_tanda_tak_wajar'] = 0\n",
    "            \n",
    "        kata_judul_list = [w for w in kata_judul_token if not w in stop_words]\n",
    "        temp2, temp3 = 0, 0\n",
    "        for w in kata_judul_list:\n",
    "            temp3 += 1\n",
    "            if w.isupper() == True:\n",
    "                temp2 += 1\n",
    "        if temp2 == temp3:\n",
    "            df_baru_return.loc[index, 'judul_kalimat_kapital'] = 1\n",
    "        elif temp2 != temp3:\n",
    "            df_baru_return.loc[index, 'judul_kalimat_kapital'] = 0\n",
    "        kata_narasi_list = [w for w in kata_narasi_token if not w in stop_words]\n",
    "        temp2, temp3 = 0, 0\n",
    "        for w in kata_narasi_list:\n",
    "            temp3 += 1\n",
    "            if w.isupper() == True:\n",
    "                temp2 += 1\n",
    "        if temp2 == temp3:\n",
    "            df_baru_return.loc[index, 'narasi_kalimat_kapital'] = 1\n",
    "        elif temp2 != temp3:\n",
    "            df_baru_return.loc[index, 'narasi_kalimat_kapital'] = 0\n",
    "\n",
    "        # Menghilangkan stopwords\n",
    "        kata_judul_notstopw = [w.lower() for w in kata_judul_token if not w in stop_words]\n",
    "        kata_narasi_notstopw = [w.lower() for w in kata_narasi_token if not w in stop_words]\n",
    "        temp4, temp5 = 0, 0\n",
    "        for w in kata_narasi_notstopw:\n",
    "            temp4 += 1\n",
    "            if w in kata_judul_notstopw:\n",
    "                temp5 += 1\n",
    "        df_baru_return.loc[index, 'jumlah_keyword'] = temp5\n",
    "        df_baru_return.loc[index, 'persentase_keyword'] = (temp5/temp4)*100\n",
    "\n",
    "        df_baru_return.loc[index, 'label'] = label[index]\n",
    "    return df_baru_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baru_train = ParamSendiri(df_judul, df_narasi, df_label, stop_words)\n",
    "df_baru_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baru_train['label'] = df_baru_train['label'].astype(int)\n",
    "display(len(df_baru_train[df_baru_train['judul_jumlah_tanda']>df_baru_train['judul_jumlah_kata']]))\n",
    "display(len(df_baru_train[df_baru_train['judul_kalimat_kapital'] == 1]))\n",
    "display(df_baru_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baru_test = ParamSendiri(df_t_judul, df_t_narasi, df_t_label, stop_words)\n",
    "df_baru_test['label'] = df_baru_test['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_baru_train.head(2))\n",
    "display(df_baru_test.head(2))\n",
    "\n",
    "df_baru_train = df_baru_train.drop(['judul_jumlah_tanda', 'narasi_jumlah_tanda', \n",
    "                                    'judul_jumlah_kata', 'narasi_jumlah_kata', 'jumlah_keyword'], axis=1)\n",
    "df_baru_test = df_baru_test.drop(['judul_jumlah_tanda', 'narasi_jumlah_tanda',\n",
    "                                  'judul_jumlah_kata', 'narasi_jumlah_kata', 'jumlah_keyword'], axis=1)\n",
    "\n",
    "display(df_baru_train.head(2))\n",
    "display(df_baru_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_baru_train.drop('label', axis=1)\n",
    "y_train = df_baru_train['label']\n",
    "x_test = df_baru_test.drop('label', axis=1)\n",
    "y_test = df_baru_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tfidf_train_df.head(2))\n",
    "display(x_train.head(2))\n",
    "print('tfidf train:', len(tfidf_train_df))\n",
    "print('x train:', len(x_train))\n",
    "X_train = pd.concat([tfidf_train_df, x_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tfidf_test_df.head(2))\n",
    "display(x_test.head(2))\n",
    "print('tfidf test:', len(tfidf_test_df))\n",
    "print('x test:', len(x_test))\n",
    "X_test = pd.concat([tfidf_test_df, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.head(2))\n",
    "display(X_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "score = metrics.f1_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1_x, subset1_y = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAC PAKE DF\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# PASSIVE-AGGRESSIVE CLASSIFIER TF-IDF   \n",
    "\n",
    "pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(subset1_x,subset1_y)\n",
    "#Predict on the test set\n",
    "y_pred1 = pac.predict(X_test)\n",
    "\n",
    "#F1 Score\n",
    "scoref1 = f1_score(y_test, y_pred1)\n",
    "print(f'F1 Score: {round(scoref1*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2_x, subset2_y = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(subset2_x, subset2_y)\n",
    "\n",
    "# Predict the labels: pred\n",
    "y_pred2 = nb_classifier.predict(X_test)\n",
    "\n",
    "# Compute accuracy: score\n",
    "score2 = metrics.f1_score(y_test, y_pred2)\n",
    "print(f'F1 Score: {round(score2*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset3_x, subset3_y = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(subset3_x, subset3_y)\n",
    "y_pred3 = logreg.predict(X_test)\n",
    "score3 = metrics.f1_score(y_test, y_pred3)\n",
    "print(f'F1 Score: {round(score3*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset5_x, subset5_y = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(subset5_x, subset5_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred5 = clf.predict(X_test)\n",
    "score5 = metrics.f1_score(y_test, y_pred5)\n",
    "print(f'F1 Score: {round(score5*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(tfidf_test_df)):\n",
    "    final_pred = np.append(final_pred, mode([y_pred1[i], y_pred2[i], y_pred3[i], pred[i], y_pred5[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_akhir = metrics.f1_score(y_test, final_pred)\n",
    "print(f'F1 Score: {round(score_akhir*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
